batchsize: 64
best acc: 0.8748
data:
  end_idx: 10000
  maxlen: 256
  start_idx: 0
epoch: 5
lr: 0.001
model:
  activation: gelu
  dim_feedforward: 32
  drop_out: 0.35
  embed_dim: 16
  max_norm: 1
  max_words: 10000
  maxlen: 256
  name: transformers
  nhead: 8
  num_layers: 2
n_epoch: 3
num_worker: 4
optimizer: Adam
scheduler:
  gamma: 0.1
  name: ReduceLROnPlateau
  patience: 2
  step_size: 16
weight_decay: 0.0001
